---
title:  "[파이썬][머신러닝] 데이터 분석과 머신러닝 - Boosting"
excerpt: "앙상블 부스팅 기법에 대해"

categories:
  - 데이터 분석과 머신러닝
tags:
  - [데이터 분석, 머신러닝]

toc: true
toc_sticky: true
 
date: 2021-12-04
last_modified_at: 2021-12-04
---

# 앙상블과 부스팅이란?
*Ensemble Learning은 모델 여러 개를 조합하여 더 정확한 예측에 도움을 주는 학습방법이다.* <br>

**앙상블은 여러 모델이 같이 문제를 해결하는 집단지성이다.**

> 1. Voting (보팅) <br>
> - 여러 개의 분류기 투표로 최종 예측 결과를 결정하는 방식 <br>
> - 각기 다른 알고리즘을 결합하여 사용하는 방식 <br>
&nbsp;  1-1. Hard Voting <br>
&nbsp; &nbsp; 다수의 분류기가 예측한 결과값을 최종 결과로 선정하는 방법 <br> 
&nbsp;  1-2. Soft Voting <br>
&nbsp; &nbsp; 모든 분류기가 예측한 레이블 값의 결정 확률 평균을 구한 다음 확률이 높은 레이블 값을 최종 결과로 선정하는 방법 <br> 
<br> 
> 2. Boostrap AGGregatING, Bagging (배깅)
> - 여러 개의 분류기 투표로 최종 예측 결과를 결정하는 방식 <br>
> - Boostrap (데이터 샘플링하는 작업)을 통해서 모델 학습 후 
결과를 집계하는 방법 <br>
> - 비슷한 유형의 알고리즘 기반의 분류기를 사용 <br>
> - 과적합 방지에 효율적이며 랜덤 포레스트 모델에서 사용하는 방법이다. <br>
<br>  
> 3. Boosting (부스팅)
> - 보팅과 비슷한 방식으로 여러 분류기를 사용하되 순차적으로 학습을 수행하는 방식을 말한다. <br>
> - 예측 성능이 매우 좋아서 앙상블 학습을 주도하는 방법이다. <br>
> - 배깅에 비해서 속도가 느리고 과적합 가능성이 있지만 성능이 좋다. <br>
> - 각각의 분류기들이 이전 분류기에서 틀린 가중치를 기반으로 다음 분류기에게 전달하여 학습과 예측을 진행한다. <br>
> - 대표적인 모델 2가지 <br>
&nbsp;  3-1. XGBoost <br>
&nbsp;  3-2. LightBGM <br>

## XGBoost

Gradient Boosting Decision Tree 구현 라이브러리의 일종으로 현재도 많이 이용되는 모델이다.

### XGBoost 간단하게 구현하기
사이킷런의 파이프라인을 활용해서 빠르게 구현해보겠다.

> 파이프라인 (Pipeline)은 전처리의 단계, 모델 생성 및 학습을 포함하는 여러 단계의 머신러닝 프로세스를 처리할 수 있는 클래스이다.



```python
from category_encoders import OrdinalEncoder
from sklearn.impute import SimpleImputer
from xgboost import XGBClassifier
from sklearn.pipeline import make_pipeline


model = make_pipeline(
    OrdinalEncoder(),
    SimpleImputer(strategy="median"),
    XGBClassifier(
        objective="binary:logistic",
        eval_metric="error", n_estimators=200,
        random_state=42,
        n_jobs=-1,
        max_depth=7,
        learning_rate=0.1,
    ),
)

```

