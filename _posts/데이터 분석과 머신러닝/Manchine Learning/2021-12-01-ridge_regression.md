---
title:  "데이터 분석과 머신러닝 - Ridge Regression"
excerpt: "One-Hot 인코딩과 특성 공학에 대해"

categories:
  - 데이터 분석과 머신러닝
tags:
  - [데이터 분석, 머신러닝]

toc: true
toc_sticky: true
 
date: 2021-12-01
last_modified_at: 2021-12-01
---

# Ridge Regression
    '능형 회귀'라고도 불리며, 기본 선형 모델에서 자주 발생하는 Overfitting을 막아주기위해 '각 계수의 제곱을 더한 값'을 식에 포함하여 계수의 크기도 함께 최소화하도록 만든 기법이다. Overfitting이 되면 데이터에 매우 적합되어서 극단적인 올라갔다 내려하는 그래프가 생기는데 이런 Variance가 큰 상황을 막기 위해 계수 자체를 커지면 패널티를 주식 수식을 추가한 것이다.

### 특징
1. 람다가 크면 계수를 많이 줄이고, 람다가 작으면 최소 제곱 문제를 풀게되는 형식이다.
2. 계수의 크기가 Shrink(수축)되는 효과가 생기는 것을 볼다.
3. 베타 제곱을 사용하기때문에 어떤 계수가 3. 덜 중요하더라도 완전히 0에 수렴하지 않고 충분히 작은 소수점에 남아있는다.

### Ridge Regression 사용시 주의점

- 변수의 크기가 결과에 큰 영향을 미친다.

그렇기에 스케일링을 해주어서 변수들의 사이즈를 비슷하게 만들어줘야하며, 일반적으로 표준편차를 각 predictors마다 구해서 각 데이터에 나눠준다.

- 람다값이 바뀌면 Bias - Variance Tradeoff

이전 Multi Regression에서 설명했던 트레이드 오프를 볼 수 있다. 람다가 작을 수록 계수 크기에 영향을 덜 미치고 좀 더 Flexible한 모델이 형성되어 Bias 문제가 적고 Variance 문제가 생긴다. 반대로 람다가 커지면 영향을 더 미치고 Bias 문제가 많고 Variance 문제가 적다.

## One-Hot Encoding

*'컴퓨터나 기계는 문자보다 숫자를 더 좋아한다.'*

그렇기에 문자를 숫자로 바꾸는 수많은 기법중 하나로 단어를 표현하는 가장 기본적인 표현 방법이다.
- 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식


## Feature Engineering (특성 공학)

    머신러닝에서 훈련에 사용할 좋은 데이터들을 찾는 방법으로, 에러, 이상치 등등 성능에 악영향을 없애는 작업을 말한다.

### 특성 공학의 종류

1. Feature Selection (특성 선택)
    - 보유한 특성 중에서 훈련에 가장 유용한 특성을 선택
    - 대게 회귀모델같은 알고리즘에서 특징을 선택하기 위해서 많이 사용한다.

2. Feature Extraction (차원 감소)
    - 특성을 결합하여 더 유용한 특성을 만들기

- **Feature Engineering 방법**
    
    1. ㅇㄴ

### 처리 방법

- 브레인스토밍
- 생성하고자 하는 특징 결정
- 특징 생성
- 모델에서의 특징 작동 여부와 방식 파악
- 필요시 특징 개선
